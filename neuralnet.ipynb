{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhanhl0902/TLS/blob/main/neuralnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJh_qoIU9quR",
        "outputId": "f95e2dd3-5aaf-4e77-9876-24b35b3c7450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading mnist-in-csv, 15970596 bytes compressed\n",
            "[==================================================] 15970596 bytes downloaded\n",
            "Downloaded and uncompressed: mnist-in-csv\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'mnist-in-csv:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F27352%2F34877%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240420%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240420T144348Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5733f2583c66134ab0983ba5027708159ba3028e3a3e5b35cc9ef9798ec092b57f28f07232b95a163d71c60f0e866f1b615d1ef189d51d1e43ed8994e016dc960880a3944e50c18fc457611f1dfe8701c2f40d952819e357b55dd3c04a563b97d83d859ca21f5ccd74f3216d97dfaf3c1489507660fe6d179a7588efd81eb841d58e879319d27b6066015d7f01e25c228f3caf91e8fe84fb27e767110c5ec4a356186933f68ac6ad3c766a6b300f7cb449493bda662cf34049761a283b5bf4265e51cea3d9a998b1b89dc240b9347f04ae4c07fe0141bc7d937ad5245883b0276e7ac1c841b104d61fb8ceb34fc9e2c194093515cb8312072b785859fae0eaaa'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQnk3jwN9quS",
        "outputId": "10efe67f-91ea-459a-832c-06e6cf16665f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/kaggle/input/mnist-in-csv/mnist_test.csv\n",
            "/kaggle/input/mnist-in-csv/mnist_train.csv\n"
          ]
        }
      ],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "_ehtIEdG9quS",
        "outputId": "d09b99b5-dff8-41ff-93f6-7f0b884b6cad"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'create_mini_batches' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ee12d74c220a>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mB2_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmini_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_mini_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmini_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmini_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_mini_batches' is not defined"
          ]
        }
      ],
      "source": [
        "def check_grad(fn, gr, X):\n",
        "    X_flat = X.reshape(-1) # convert X to an 1d array, 1 for loop needed\n",
        "    shape_X = X.shape # original shape of X\n",
        "    num_grad = np.zeros_like(X) # numerical grad, shape = shape of X\n",
        "    grad_flat = np.zeros_like(X_flat) # 1d version of grad\n",
        "    eps = 1e-6# a small number, 1e-10 -> 1e-6 is usually good\n",
        "    numElems = X_flat.shape[0] # number of elements in X\n",
        "# calculate numerical gradient\n",
        "    for i in range(numElems): # iterate over all elements of X\n",
        "        Xp_flat = X_flat.copy()\n",
        "        Xn_flat = X_flat.copy()\n",
        "        Xp_flat[i] += eps\n",
        "        Xn_flat[i] -= eps\n",
        "        Xp = Xp_flat.reshape(shape_X)\n",
        "        Xn = Xn_flat.reshape(shape_X)\n",
        "        grad_flat[i] = (fn(Xp) - fn(Xn))/(2*eps)\n",
        "    num_grad = grad_flat.reshape(shape_X)\n",
        "    diff = np.linalg.norm(num_grad - gr(X))\n",
        "    print('Difference between two methods should be small:', diff)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "def gd_of_sigmoid(x):\n",
        "    return sigmoid(x)*(1-sigmoid(x))\n",
        "np.random.seed(10)\n",
        "W0_test= np.random.rand(16,784)\n",
        "B0_test=np.random.rand(1,16)\n",
        "W1_test=np.random.rand(17,16)\n",
        "B1_test=np.random.rand(1,17)\n",
        "W2_test=np.random.rand(10,17)\n",
        "B2_test=np.random.rand(1,10)\n",
        "batch_size = 100\n",
        "mini_batches = create_mini_batches(df, batch_size)\n",
        "label=mini_batches[0]['label'].values\n",
        "X=mini_batches[0].drop(columns='label').values/255\n",
        "onehot=vector_to_one_hot(label,10)\n",
        "def fn1(W2_test):\n",
        "    a0=sigmoid(X @ W0_test.T+B0_test)\n",
        "    a1=sigmoid(a0 @ W1_test.T +B1_test)\n",
        "    a2=sigmoid(a1 @ W2_test.T +B2_test)\n",
        "    return (1/200)*(np.linalg.norm(a2-onehot)**2)\n",
        "def gr1(W2_test):\n",
        "    a0=sigmoid(X @ W0_test.T+B0_test)\n",
        "    a1=sigmoid(a0 @ W1_test.T +B1_test)\n",
        "    a2=sigmoid(a1 @ W2_test.T +B2_test)\n",
        "    one2=np.ones(10)\n",
        "    return np.mean(np.einsum('ij,ik->ikj',a1,((a2*(one2-a2))*(a2-onehot))),axis=0)\n",
        "def fn2(W1_test):\n",
        "    a0=sigmoid(X @ W0_test.T+B0_test)\n",
        "    a1=sigmoid(a0 @ W1_test.T +B1_test)\n",
        "    a2=sigmoid(a1 @ W2_test.T +B2_test)\n",
        "    return (1/200)*(np.linalg.norm(a2-onehot)**2)\n",
        "def gr2(W1_test):\n",
        "    a0=sigmoid(X @ W0_test.T+B0_test)\n",
        "    a1=sigmoid(a0 @ W1_test.T +B1_test)\n",
        "    a2=sigmoid(a1 @ W2_test.T +B2_test)\n",
        "    one2=np.ones(10)\n",
        "    one1=np.ones(17)\n",
        "    return np.mean(np.einsum('ij,ik->ikj',a0,((a1*(one1-a1))*np.einsum('ij,ki->kj',W2_test,((a2*(one2-a2))*(a2-onehot))))),axis=0)\n",
        "def fn3(W0_test):\n",
        "    a0=sigmoid(X @ W0_test.T+B0_test)\n",
        "    a1=sigmoid(a0 @ W1_test.T +B1_test)\n",
        "    a2=sigmoid(a1 @ W2_test.T +B2_test)\n",
        "    return (1/200)*(np.linalg.norm(a2-onehot)**2)\n",
        "def gr3(W0_test):\n",
        "    a0=sigmoid(X @ W0_test.T+B0_test)\n",
        "    a1=sigmoid(a0 @ W1_test.T +B1_test)\n",
        "    a2=sigmoid(a1 @ W2_test.T +B2_test)\n",
        "    one2=np.ones(10)\n",
        "    one1=np.ones(17)\n",
        "    one0=np.ones(16)\n",
        "    return np.mean(np.einsum('ij,ik->ikj',X,(a0*(one0-a0))*(np.einsum('ij,ki->kj',W1_test,(a1*(one1-a1))*np.einsum('ij,ki->kj',W2_test,((a2*(one2-a2))*(a2-onehot)))))),axis=0)\n",
        "def fn4(B2_test):\n",
        "    a0=sigmoid(X @ W0_test.T+B0_test)\n",
        "    a1=sigmoid(a0 @ W1_test.T +B1_test)\n",
        "    a2=sigmoid(a1 @ W2_test.T +B2_test)\n",
        "    return (1/200)*(np.linalg.norm(a2-onehot)**2)\n",
        "def gr4(B2_test):\n",
        "    a0=sigmoid(X @ W0_test.T+B0_test)\n",
        "    a1=sigmoid(a0 @ W1_test.T +B1_test)\n",
        "    a2=sigmoid(a1 @ W2_test.T +B2_test)\n",
        "    one2=np.ones(10)\n",
        "    return np.mean((a2*(one2-a2))*(a2-onehot),axis=0)\n",
        "def fn5(B1_test):\n",
        "    a0=sigmoid(X @ W0_test.T+B0_test)\n",
        "    a1=sigmoid(a0 @ W1_test.T +B1_test)\n",
        "    a2=sigmoid(a1 @ W2_test.T +B2_test)\n",
        "    return (1/200)*(np.linalg.norm(a2-onehot)**2)\n",
        "def gr5(B1_test):\n",
        "    a0=sigmoid(X @ W0_test.T+B0_test)\n",
        "    a1=sigmoid(a0 @ W1_test.T +B1_test)\n",
        "    a2=sigmoid(a1 @ W2_test.T +B2_test)\n",
        "    one2=np.ones(10)\n",
        "    one1=np.ones(17)\n",
        "\n",
        "    return (((a1*(one1-a1)).T)*(W2_test.T @ ((a2*(one2-a2))*(a2-onehot)).T)).T\n",
        "def fn6(B0_test):\n",
        "    a0=sigmoid(X @ W0_test.T+B0_test)\n",
        "    a1=sigmoid(a0 @ W1_test.T +B1_test)\n",
        "    a2=sigmoid(a1 @ W2_test.T +B2_test)\n",
        "    return 0.5*(np.linalg.norm(a2-onehot)**2)\n",
        "def gr6(B0_test):\n",
        "    a0=sigmoid(X @ W0_test.T+B0_test)\n",
        "    a1=sigmoid(a0 @ W1_test.T +B1_test)\n",
        "    a2=sigmoid(a1 @ W2_test.T +B2_test)\n",
        "    one2=np.ones(10)\n",
        "    one1=np.ones(17)\n",
        "    one0=np.ones(16)\n",
        "    return ((a0*(one0-a0)).T*(W1_test.T @ ((a1*(one1-a1)).T*(W2_test.T @ ((a2*(one2-a2))*(a2-onehot)).T)))).T\n",
        "\n",
        "\n",
        "# theoretical_derivative, estimated_derivative, error = check_derivative(fn4, gr4, x)\n",
        "\n",
        "# print(f\"Đạo hàm lý thuyết: {theoretical_derivative}\")\n",
        "# print(f\"Đạo hàm ước tính: {estimated_derivative}\")\n",
        "# print(f\"Lỗi tuyệt đối: {error}\")\n",
        "\n",
        "check_grad(fn1,gr1,W2_test)\n",
        "check_grad(fn2,gr2,W1_test)\n",
        "check_grad(fn3,gr3,W0_test)\n",
        "check_grad(fn4,gr4,B2_test)\n",
        "# check_grad(fn5,gr5,B1_test)\n",
        "# check_grad(fn6,gr6,B0_test)\n",
        "# a=fn1(W2_test)\n",
        "# print(a.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltPW8FAf9quT"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "test='/kaggle/input/mnist-in-csv/mnist_test.csv'\n",
        "train='/kaggle/input/mnist-in-csv/mnist_train.csv'\n",
        "df=pd.read_csv(train)\n",
        "def create_mini_batches(dataframe, batch_size):\n",
        "    num_batches = len(dataframe) // batch_size\n",
        "    mini_batches = []\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = start_idx + batch_size\n",
        "        mini_batch = dataframe.iloc[start_idx:end_idx]\n",
        "        mini_batches.append(mini_batch)\n",
        "\n",
        "    # If there are any remaining data points, create a final mini-batch\n",
        "    if len(dataframe) % batch_size != 0:\n",
        "        mini_batch = dataframe.iloc[num_batches * batch_size:]\n",
        "        mini_batches.append(mini_batch)\n",
        "\n",
        "    return mini_batches\n",
        "\n",
        "\n",
        "# For a scalar\n",
        "def scalar_to_one_hot(scalar, num_classes):\n",
        "    return to_categorical(scalar, num_classes=num_classes)\n",
        "# For a vector\n",
        "def vector_to_one_hot(vector, num_classes):\n",
        "    return to_categorical(vector, num_classes=num_classes)\n",
        "\n",
        "# Assuming your dataframe is named 'df' and you want mini-batches of size 100\n",
        "batch_size = 100\n",
        "mini_batches = create_mini_batches(df, batch_size)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "def gd_of_sigmoid(x):\n",
        "    return sigmoid(x)*(1-sigmoid(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BuUltty9quU"
      },
      "outputs": [],
      "source": [
        "def initialize_weights_xavier(input_size, output_size):\n",
        "    \"\"\"\n",
        "    Khởi tạo trọng số theo phương pháp Xavier.\n",
        "\n",
        "    Tham số:\n",
        "    input_size: Số lượng neuron trong lớp đầu vào.\n",
        "    output_size: Số lượng neuron trong lớp đầu ra.\n",
        "\n",
        "    Trả về:\n",
        "    Trọng số được khởi tạo theo phương pháp Xavier.\n",
        "    \"\"\"\n",
        "    # Tính độ lệch chuẩn theo công thức của Xavier\n",
        "    std_dev = np.sqrt(2 / (input_size + output_size))\n",
        "    # Khởi tạo trọng số ngẫu nhiên từ phân phối chuẩn (mean=0, std=std_dev)\n",
        "    weights = np.random.normal(loc=0, scale=std_dev, size=(input_size, output_size))\n",
        "    return weights\n",
        "\n",
        "w_0= initialize_weights_xavier(784,16).T\n",
        "b_0=np.random.rand(1,16)\n",
        "w_1=initialize_weights_xavier(16,17).T\n",
        "b_1=np.random.rand(1,17)\n",
        "w_2=initialize_weights_xavier(17,10).T\n",
        "b_2=np.random.rand(1,10)\n",
        "one_2=np.ones(10)\n",
        "one_1=np.ones(17)\n",
        "one_0=np.ones(16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO4esB2MCMmt"
      },
      "outputs": [],
      "source": [
        "index=0\n",
        "label=mini_batches[index]['label'].values\n",
        "X=mini_batches[index].drop(columns='label').values/255\n",
        "onehot=vector_to_one_hot(label,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5bbCf4C9quU"
      },
      "outputs": [],
      "source": [
        "df_test=pd.read_csv(test)\n",
        "X_test = df_test.drop(columns=['label']).values/255\n",
        "y_test = df_test['label'].values\n",
        "oh_test=vector_to_one_hot(y_test,10)\n",
        "\n",
        "\n",
        "for index in range(0,600):\n",
        "\n",
        "\n",
        "  a_0_test=(X_test @ w_0.T+b_0)\n",
        "  a_1_test=sigmoid(a_0_test @ w_1.T+b_1)\n",
        "  a_2_test=sigmoid(a_1_test @ w_2.T+b_2)\n",
        "\n",
        "  predictions=np.argmax(a_2_test,axis=1)\n",
        "  accuracy = np.mean(y_test == predictions)\n",
        "\n",
        "\n",
        "  # index=0\n",
        "  label=mini_batches[index]['label'].values\n",
        "  X=mini_batches[index].drop(columns='label').values/255\n",
        "  onehot=vector_to_one_hot(label,10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  validate_index=np.random.randint(0,599)\n",
        "  validate_X=mini_batches[validate_index].drop(columns='label')/255\n",
        "  validate_y=mini_batches[validate_index]['label']\n",
        "  validate_one_hot=vector_to_one_hot(validate_y,10)\n",
        "  a=0\n",
        "\n",
        "\n",
        "  while a<1000:\n",
        "      a_0=sigmoid(X @ w_0.T+b_0)\n",
        "      a_1=sigmoid(a_0 @ w_1.T+b_1)\n",
        "      a_2=sigmoid(a_1 @ w_2.T+b_2)\n",
        "      dist=(1/200)*(np.linalg.norm(a_2-onehot)**2)\n",
        "\n",
        "\n",
        "\n",
        "      a_0_test=(X_test @ w_0.T+b_0)\n",
        "      a_1_test=sigmoid(a_0_test @ w_1.T+b_1)\n",
        "      a_2_test=sigmoid(a_1_test @ w_2.T+b_2)\n",
        "      test_error=(1/10000)*0.5*(np.linalg.norm(a_2_test-oh_test)**2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      validate_a_0=(validate_X @ w_0.T+b_0)\n",
        "      validate_a_1=sigmoid(validate_a_0 @ w_1.T+b_1)\n",
        "      validate_a_2=sigmoid(validate_a_1 @ w_2.T+b_2)\n",
        "      validate_error=(1/200)*(np.linalg.norm(validate_a_2-validate_one_hot)**2)\n",
        "\n",
        "\n",
        "      print(dist,'Validate_error:',validate_error,'Error_test:',test_error,\"Accuracy:\",accuracy*100,'%')\n",
        "\n",
        "\n",
        "\n",
        "      gd_of_w_2=np.mean(np.einsum('ij,ik->ikj',a_1,((a_2*(one_2-a_2))*(a_2-onehot))),axis=0)\n",
        "      gd_of_w_1=np.mean(np.einsum('ij,ik->ikj',a_0,((a_1*(one_1-a_1))*np.einsum('ij,ki->kj',w_2,((a_2*(one_2-a_2))*(a_2-onehot))))),axis=0)\n",
        "      gd_of_w_0=np.mean(np.einsum('ij,ik->ikj',X,(a_0*(one_0-a_0))*(np.einsum('ij,ki->kj',w_1,(a_1*(one_1-a_1))*np.einsum('ij,ki->kj',w_2,((a_2*(one_2-a_2))*(a_2-onehot)))))),axis=0)\n",
        "      gd_of_b_2=np.mean((a_2*(one_2-a_2))*(a_2-onehot),axis=0)\n",
        "      gd_of_b_1=np.mean((a_1*(one_1-a_1))*(np.einsum('ij,ki->kj',w_2,((a_2*(one_2-a_2))*(a_2-onehot)))),axis=0)\n",
        "      gd_of_b_0=np.mean((a_0*(one_0-a_0))*np.einsum('ij,ki->kj',w_1,(a_1*(one_1-a_1))*np.einsum('ij,ki->kj',w_2,((a_2*(one_2-a_2))*(a_2-onehot)))),axis=0)\n",
        "\n",
        "\n",
        "\n",
        "      learning_rate=10\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      w_0=w_0-learning_rate*gd_of_w_0\n",
        "      w_1=w_1-learning_rate*gd_of_w_1\n",
        "      w_2=w_2-learning_rate*gd_of_w_2\n",
        "      b_0=b_0-learning_rate*gd_of_b_0\n",
        "      b_1=b_1-learning_rate*gd_of_b_1\n",
        "      b_2=b_2-learning_rate*gd_of_b_2\n",
        "      a+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "nDECwe4Z9quV",
        "outputId": "855fe64b-f708-43a5-efa1-364fc5687628"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/weight_model/weight_0.npy'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-873afc46a5b2>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Save weights to a .npy file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/content/drive/MyDrive/weight_model/weight_{i}.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mfile_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/weight_model/weight_0.npy'"
          ]
        }
      ],
      "source": [
        "weights = [w_0, w_1, w_2, b_0, b_1, b_2]\n",
        "\n",
        "# Save weights to a .npy file.npy', weight)\n",
        "\n",
        "\n",
        "loaded_weights = [np.load(f'/content/drive/MyDrive/weight_model/weight_{i}.npy', al\n",
        "for i, weight in enumerate(weights):\n",
        "    np.save(f'/content/drive/MyDrive/weight_model/weight_{i}low_pickle=True) for i in range(len(weights))]\n",
        "print(loaded_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_0JctuaHdzp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NKCit3rCwV7",
        "outputId": "1e9dba4d-d5d3-4ffc-d7d3-38cf2af95cbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.109665725306228\n"
          ]
        }
      ],
      "source": [
        "# df_test=pd.read_csv(test)\n",
        "# X_test = df_test.drop(columns=['label']).values/255\n",
        "# y_test = df_test['label'].values\n",
        "\n",
        "\n",
        "# a_0_test=(X_test @ w_0.T+b_0)\n",
        "# a_1_test=sigmoid(a_0_test @ w_1.T+b_1)\n",
        "# a_2_test=sigmoid(a_1_test @ w_2.T+b_2)\n",
        "# oh_test=vector_to_one_hot(y_test,10)\n",
        "\n",
        "# predictions=np.argmax(a_2_test,axis=1)\n",
        "\n",
        "# accuracy = np.mean(y_test == predictions)\n",
        "\n",
        "# print(\"Accuracy:\", accuracy*100,'%')\n",
        "# test_error=(1/10000)*0.5*(np.linalg.norm(a_2_test-oh_test)**2)\n",
        "i=np.random.randint(0,599)\n",
        "validate_X=mini_batches[i].drop(columns='label')/255\n",
        "validate_y=mini_batches[i]['label']\n",
        "validate_a_0=(validate_X @ w_0.T+b_0)\n",
        "validate_a_1=sigmoid(validate_a_0 @ w_1.T+b_1)\n",
        "validate_a_2=sigmoid(validate_a_1 @ w_2.T+b_2)\n",
        "validate_one_hot=vector_to_one_hot(validate_y,10)\n",
        "validate_error=(1/200)*(np.linalg.norm(validate_a_2-validate_one_hot)**2)\n",
        "print(validate_error)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 27352,
          "sourceId": 34877,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30684,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}